<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration supports_final="true">
  <property>
    <name>dfs.namenode.audit.log.async</name>
    <value>true</value>
    <description>Whether to enable async auditlog</description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.namenode.fslock.fair</name>
    <value>false</value>
    <description>Whether fsLock is fair</description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.namenode.startup.delay.block.deletion.sec</name>
    <value>3600</value>
    <description>
      The delay in seconds at which we will pause the blocks deletion
      after Namenode startup. By default it's disabled.
      In the case a directory has large number of directories and files are
      deleted, suggested delay is one hour to give the administrator enough time
      to notice large number of pending deletion blocks and take corrective
      action.
    </description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/hadoop/hdfs/journalnode</value>
    <description>The path where the JournalNode daemon will store its local state. </description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.client.retry.policy.enabled</name>
    <value>false</value>
    <description>Enables HDFS client retry in the event of a NameNode failure.</description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.content-summary.limit</name>
    <value>5000</value>
    <description>Dfs content summary limit.</description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.encryption.key.provider.uri</name>
    <description>
      The KeyProvider to use when interacting with encryption keys used
      when reading and writing to an encryption zone.
    </description>
    <value/>
    <value-attributes>
      <empty-value-valid>true</empty-value-valid>
    </value-attributes>
    <depends-on>
      <property>
        <type>hadoop-env</type>
        <name>keyserver_host</name>
      </property>
      <property>
        <type>hadoop-env</type>
        <name>keyserver_port</name>
      </property>
      <property>
        <type>kms-env</type>
        <name>kms_port</name>
      </property>
      <property>
        <type>ranger-kms-site</type>
        <name>ranger.service.https.attrib.ssl.enabled</name>
      </property>
    </depends-on>
    <on-ambari-upgrade add="false"/>
  </property>
  
  
  
  
  <property>
    <name>nfs.file.dump.dir</name>
    <value>/tmp/.hdfs-nfs</value>
    <display-name>NFSGateway dump directory</display-name>
    <description>
      This directory is used to temporarily save out-of-order writes before
      writing to HDFS. For each file, the out-of-order writes are dumped after
      they are accumulated to exceed certain threshold (e.g., 1MB) in memory.
      One needs to make sure the directory has enough space.
    </description>
    <value-attributes>
      <type>directory</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>nfs.exports.allowed.hosts</name>
    <value>* rw</value>
    <description>
      By default, the export can be mounted by any client. To better control the access,
      users can update the following property. The value string contains machine name and access privilege,
      separated by whitespace characters. Machine name format can be single host, wildcards, and IPv4
      networks.The access privilege uses rw or ro to specify readwrite or readonly access of the machines
      to exports. If the access privilege is not provided, the default is read-only. Entries are separated
      by &quot;;&quot;. For example: &quot;192.168.0.0/22 rw ; host*.example.com ; host1.test.org ro;&quot;.
    </description>
    <display-name>Allowed hosts</display-name>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.encrypt.data.transfer.cipher.suites</name>
    <value>AES/CTR/NoPadding</value>
    <description>
      This value may be either undefined or AES/CTR/NoPadding. If defined, then 
      dfs.encrypt.data.transfer uses the specified cipher suite for data encryption. 
      If not defined, then only the algorithm specified in dfs.encrypt.data.transfer.algorithm 
      is used. By default, the property is not defined.
    </description>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>dfs.namenode.inode.attributes.provider.class</name>
    <description>Enable ranger hdfs plugin</description>
    <depends-on>
      <property>
        <type>ranger-hdfs-plugin-properties</type>
        <name>ranger-hdfs-plugin-enabled</name>
      </property>
    </depends-on>
    <value-attributes>
      <overridable>false</overridable>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
  <property>
    <name>hadoop.caller.context.enabled</name>
    <value>true</value>
    <on-ambari-upgrade add="false"/>
  </property>
</configuration>
